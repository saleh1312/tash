{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyarabic.araby import strip_harakat , strip_tashkeel, strip_diacritics,strip_tatweel,sentence_tokenize\n",
    "import re\n",
    "import string\n",
    "import grapheme\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tokenizer' from 'd:\\\\arabic_tashkeela\\\\tokenizer.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "import utils\n",
    "import tokenizer\n",
    "\n",
    "importlib.reload(utils)\n",
    "importlib.reload(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/إتحاف المهرة لابن حجر.txt\") as file:\n",
    "    lines = file.readlines()\n",
    "data1 = lines[3:]\n",
    "\n",
    "with open(\"dataset/أحكام القرآن لابن العربي.txt\") as file:\n",
    "    lines = file.readlines()\n",
    "data2 = lines[6:]\n",
    "\n",
    "with open(\"dataset/إتحاف المهرة لابن حجر.txt\") as file:\n",
    "    lines = file.readlines()\n",
    "data3 = lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    *data1,\n",
    "    *data2,\n",
    "    *data3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "let , tash = utils.extract_let_tash(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'حَدِيثٌ كم حم: أَنَّهُ رَأَى رَسُولَ اللَّهِ، صَلَّى اللَّهُ عَلِيهِ وَسَلَّم، عِنْدَ أَحْجَارِ الزَّيْتِ يَسْتَسْقِي مُقَنَّعًا بِكَفَّيْهِ يَدْعُو، هَكَذَا كم فِي الاسْتِسْقَاءِ: حَدَّثَنَا عَلِيُّ بْنُ حَمْشَاذَ، حَدَّثَنَا عُبَيْدُ بْنُ شَرِيكٍ، حَدَّثَنَا يَحْيَى بْنُ بُكَيْرٍ، عَنِ اللَّيْثِ، عَنْ خَالِدِ بْنِ يَزِيدَ، عَنْ سَعِيدِ بْنِ أَبِي هِلالٍ، عَنْ يَزِيدَ بْنِ عَبْدِ اللَّهِ، هُوَ ابْنُ الْهَادِ عَنْ مُحَمَّدِ بْنِ إِبْرَاهِيمَ، عَنْ عُمَيْرٍ، مَوْلَى آبِي اللَّحْمِ، عَنْهُ بِهَذَا، وَقَالَ: صَحِيحُ الإِسْنَادِ، وَفِي الدُّعَاءِ: حَدَّثَنَا أَبُو الْعَبَّاسِ مُحَمَّدُ بْنُ يَعْقُوبَ، حَدَّثَنَا مُحَمَّدُ بْنُ عَبْدِ اللَّهِ بْنِ عَبْدِ الْحَكَمِ، حَدَّثَنَا أَبِي، وَشُعَيْبُ بْنُ اللَّيْثِ، قَالا: حَدَّثَنَا اللَّيْثُ، بِهِ وَرَوَاهُ أَحْمَدُ: عَنْ قُتَيْبَةَ، عَنِ اللَّيْثِ، بِهِ.(1/1)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['َ', 'ِ', ' ', 'ٌ', ' ', ' ', ' ', ' ', ' ', ' ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tash[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ح', 'د', 'ي', 'ث', ' ', 'ك', 'م', ' ', 'ح', 'م']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_let , u_tash = utils.sample_unique(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "let_tok = tokenizer.Tokenizer(u_let,20)\n",
    "tash_tok = tokenizer.Tokenizer(u_tash,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 2,\n",
       " 'َّ': 3,\n",
       " 'ُ': 4,\n",
       " 'َ': 5,\n",
       " 'ٍ': 6,\n",
       " 'ُّ': 7,\n",
       " 'ً': 8,\n",
       " 'ٌ': 9,\n",
       " 'ْ': 10,\n",
       " 'ِ': 11,\n",
       " 'ِّ': 12,\n",
       " '<حشو>': 0,\n",
       " '<مجهول>': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tash_tok.char2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ح', 'د', 'ي', 'ث', ' ', 'ك', 'م', ' ', 'ح', 'م', ' ', 'أ', 'ن', 'ه', ' ', 'ر', 'أ', 'ى', ' ', 'ر', 'س', 'و', 'ل', ' ', 'ا', 'ل', 'ل', 'ه', ' ', 'ص', 'ل', 'ى', ' ', 'ا', 'ل', 'ل', 'ه', ' ', 'ع', 'ل', 'ي', 'ه', ' ', 'و', 'س', 'ل', 'م', ' ', 'ع', 'ن', 'د', ' ', 'أ', 'ح', 'ج', 'ا', 'ر', ' ', 'ا', 'ل', 'ز', 'ي', 'ت', ' ', 'ي', 'س', 'ت', 'س', 'ق', 'ي', ' ', 'م', 'ق', 'ن', 'ع', 'ا', ' ', 'ب', 'ك', 'ف', 'ي', 'ه', ' ', 'ي', 'د', 'ع', 'و', ' ', 'ه', 'ك', 'ذ', 'ا', ' ', 'ك', 'م', ' ', 'ف', 'ي', ' ', 'ا', 'ل', 'ا', 'س', 'ت', 'س', 'ق', 'ا', 'ء', ' ', 'ح', 'د', 'ث', 'ن', 'ا', ' ', 'ع', 'ل', 'ي', ' ', 'ب', 'ن', ' ', 'ح', 'م', 'ش', 'ا', 'ذ', ' ', 'ح', 'د', 'ث', 'ن', 'ا', ' ', 'ع', 'ب', 'ي', 'د', ' ', 'ب', 'ن', ' ', 'ش', 'ر', 'ي', 'ك', ' ', 'ح', 'د', 'ث', 'ن', 'ا', ' ', 'ي', 'ح', 'ي', 'ى', ' ', 'ب', 'ن', ' ', 'ب', 'ك', 'ي', 'ر', ' ', 'ع', 'ن', ' ', 'ا', 'ل', 'ل', 'ي', 'ث', ' ', 'ع', 'ن', ' ', 'خ', 'ا', 'ل', 'د', ' ', 'ب', 'ن', ' ', 'ي', 'ز', 'ي', 'د', ' ', 'ع', 'ن', ' ', 'س', 'ع', 'ي', 'د', ' ', 'ب', 'ن', ' ', 'أ', 'ب', 'ي', ' ', 'ه', 'ل', 'ا', 'ل', ' ', 'ع', 'ن', ' ', 'ي', 'ز', 'ي', 'د', ' ', 'ب', 'ن', ' ', 'ع', 'ب', 'د', ' ', 'ا', 'ل', 'ل', 'ه', ' ', 'ه', 'و', ' ', 'ا', 'ب', 'ن', ' ', 'ا', 'ل', 'ه', 'ا', 'د', ' ', 'ع', 'ن', ' ', 'م', 'ح', 'م', 'د', ' ', 'ب', 'ن', ' ', 'إ', 'ب', 'ر', 'ا', 'ه', 'ي', 'م', ' ', 'ع', 'ن', ' ', 'ع', 'م', 'ي', 'ر', ' ', 'م', 'و', 'ل', 'ى', ' ', 'آ', 'ب', 'ي', ' ', 'ا', 'ل', 'ل', 'ح', 'م', ' ', 'ع', 'ن', 'ه', ' ', 'ب', 'ه', 'ذ', 'ا', ' ', 'و', 'ق', 'ا', 'ل', ' ', 'ص', 'ح', 'ي', 'ح', ' ', 'ا', 'ل', 'إ', 'س', 'ن', 'ا', 'د', ' ', 'و', 'ف', 'ي', ' ', 'ا', 'ل', 'د', 'ع', 'ا', 'ء', ' ', 'ح', 'د', 'ث', 'ن', 'ا', ' ', 'أ', 'ب', 'و', ' ', 'ا', 'ل', 'ع', 'ب', 'ا', 'س', ' ', 'م', 'ح', 'م', 'د', ' ', 'ب', 'ن', ' ', 'ي', 'ع', 'ق', 'و', 'ب', ' ', 'ح', 'د', 'ث', 'ن', 'ا', ' ', 'م', 'ح', 'م', 'د', ' ', 'ب', 'ن', ' ', 'ع', 'ب', 'د', ' ', 'ا', 'ل', 'ل', 'ه', ' ', 'ب', 'ن', ' ', 'ع', 'ب', 'د', ' ', 'ا', 'ل', 'ح', 'ك', 'م', ' ', 'ح', 'د', 'ث', 'ن', 'ا', ' ', 'أ', 'ب', 'ي', ' ', 'و', 'ش', 'ع', 'ي', 'ب', ' ', 'ب', 'ن', ' ', 'ا', 'ل', 'ل', 'ي', 'ث', ' ', 'ق', 'ا', 'ل', 'ا', ' ', 'ح', 'د', 'ث', 'ن', 'ا', ' ', 'ا', 'ل', 'ل', 'ي', 'ث', ' ', 'ب', 'ه', ' ', 'و', 'ر', 'و', 'ا', 'ه', ' ', 'أ', 'ح', 'م', 'د', ' ', 'ع', 'ن', ' ', 'ق', 'ت', 'ي', 'ب', 'ة', ' ', 'ع', 'ن', ' ', 'ا', 'ل', 'ل', 'ي', 'ث', ' ', 'ب', 'ه', ' ']\n",
      "[6, 5, 7, 34, 18, 31, 10, 18, 6, 10, 18, 32, 22, 8, 18, 21, 32, 17, 18, 21]\n",
      "['ح', 'د', 'ي', 'ث', ' ', 'ك', 'م', ' ', 'ح', 'م', ' ', 'أ', 'ن', 'ه', ' ', 'ر', 'أ', 'ى', ' ', 'ر']\n"
     ]
    }
   ],
   "source": [
    "let , tash = utils.extract_let_tash(data[0])\n",
    "let_tokens = let_tok.sent_to_ids(let)\n",
    "let_chars = let_tok.ids_to_sent(let_tokens)\n",
    "print(let)\n",
    "print(let_tokens)\n",
    "print(let_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = random.sample(data,100)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sent = self.data[idx]\n",
    "        let , tash = utils.extract_let_tash(sent)\n",
    "        let_tokens = let_tok.sent_to_ids(let)\n",
    "        tash_tokens = tash_tok.sent_to_ids(tash)\n",
    "\n",
    "        return torch.tensor(let_tokens), torch.tensor(tash_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomImageDataset(data)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([15, 28, 23, 16, 18, 23, 16, 22, 12,  7, 18, 14, 16, 17, 18, 23, 16, 16,\n",
       "          8, 18]),\n",
       " tensor([ 5,  5,  2,  5,  2,  2,  2,  3, 11,  7,  2,  5,  3,  2,  2,  2,  2,  3,\n",
       "          4,  2]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "class TokenClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(TokenClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        logits = self.fc(lstm_out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(let_tok.char2id)\n",
    "embedding_dim = 5\n",
    "hidden_dim = 10\n",
    "output_dim = len(tash_tok.char2id)\n",
    "\n",
    "model = TokenClassifier(vocab_size, embedding_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 2.0661516785621643\n",
      "Epoch 2/100, Loss: 2.0130794048309326\n",
      "Epoch 3/100, Loss: 2.0072998702526093\n",
      "Epoch 4/100, Loss: 1.9880936741828918\n",
      "Epoch 5/100, Loss: 1.9517181515693665\n",
      "Epoch 6/100, Loss: 1.9216060638427734\n",
      "Epoch 7/100, Loss: 1.905726134777069\n",
      "Epoch 8/100, Loss: 1.8711503148078918\n",
      "Epoch 9/100, Loss: 1.866332471370697\n",
      "Epoch 10/100, Loss: 1.8483146131038666\n",
      "Epoch 11/100, Loss: 1.8331486284732819\n",
      "Epoch 12/100, Loss: 1.8203915059566498\n",
      "Epoch 13/100, Loss: 1.782901555299759\n",
      "Epoch 14/100, Loss: 1.7563607394695282\n",
      "Epoch 15/100, Loss: 1.8233104944229126\n",
      "Epoch 16/100, Loss: 1.785841315984726\n",
      "Epoch 17/100, Loss: 1.753622829914093\n",
      "Epoch 18/100, Loss: 1.7441161572933197\n",
      "Epoch 19/100, Loss: 1.7414315044879913\n",
      "Epoch 20/100, Loss: 1.7152418792247772\n",
      "Epoch 21/100, Loss: 1.716612845659256\n",
      "Epoch 22/100, Loss: 1.737771600484848\n",
      "Epoch 23/100, Loss: 1.7547439932823181\n",
      "Epoch 24/100, Loss: 1.6636933386325836\n",
      "Epoch 25/100, Loss: 1.7332665026187897\n",
      "Epoch 26/100, Loss: 1.6853596568107605\n",
      "Epoch 27/100, Loss: 1.6863491535186768\n",
      "Epoch 28/100, Loss: 1.732073813676834\n",
      "Epoch 29/100, Loss: 1.6910580098628998\n",
      "Epoch 30/100, Loss: 1.6332589089870453\n",
      "Epoch 31/100, Loss: 1.6919610500335693\n",
      "Epoch 32/100, Loss: 1.6738190650939941\n",
      "Epoch 33/100, Loss: 1.618343561887741\n",
      "Epoch 34/100, Loss: 1.6773256659507751\n",
      "Epoch 35/100, Loss: 1.661198616027832\n",
      "Epoch 36/100, Loss: 1.6674111485481262\n",
      "Epoch 37/100, Loss: 1.6327316761016846\n",
      "Epoch 38/100, Loss: 1.6682149767875671\n",
      "Epoch 39/100, Loss: 1.685271978378296\n",
      "Epoch 40/100, Loss: 1.6123274564743042\n",
      "Epoch 41/100, Loss: 1.6122888624668121\n",
      "Epoch 42/100, Loss: 1.6127065122127533\n",
      "Epoch 43/100, Loss: 1.6162716150283813\n",
      "Epoch 44/100, Loss: 1.6033520698547363\n",
      "Epoch 45/100, Loss: 1.6102504134178162\n",
      "Epoch 46/100, Loss: 1.6363877952098846\n",
      "Epoch 47/100, Loss: 1.5585014522075653\n",
      "Epoch 48/100, Loss: 1.5971418619155884\n",
      "Epoch 49/100, Loss: 1.5484537482261658\n",
      "Epoch 50/100, Loss: 1.5973655879497528\n",
      "Epoch 51/100, Loss: 1.531061738729477\n",
      "Epoch 52/100, Loss: 1.555549144744873\n",
      "Epoch 53/100, Loss: 1.5992215275764465\n",
      "Epoch 54/100, Loss: 1.5845960080623627\n",
      "Epoch 55/100, Loss: 1.539404571056366\n",
      "Epoch 56/100, Loss: 1.5694209039211273\n",
      "Epoch 57/100, Loss: 1.5240568220615387\n",
      "Epoch 58/100, Loss: 1.53780597448349\n",
      "Epoch 59/100, Loss: 1.520011842250824\n",
      "Epoch 60/100, Loss: 1.474469244480133\n",
      "Epoch 61/100, Loss: 1.5575865805149078\n",
      "Epoch 62/100, Loss: 1.5317172408103943\n",
      "Epoch 63/100, Loss: 1.5169737935066223\n",
      "Epoch 64/100, Loss: 1.4933750331401825\n",
      "Epoch 65/100, Loss: 1.5064643025398254\n",
      "Epoch 66/100, Loss: 1.4885053932666779\n",
      "Epoch 67/100, Loss: 1.5275606215000153\n",
      "Epoch 68/100, Loss: 1.4975643754005432\n",
      "Epoch 69/100, Loss: 1.5042169690132141\n",
      "Epoch 70/100, Loss: 1.4847792088985443\n",
      "Epoch 71/100, Loss: 1.4854503870010376\n",
      "Epoch 72/100, Loss: 1.41403266787529\n",
      "Epoch 73/100, Loss: 1.4300660192966461\n",
      "Epoch 74/100, Loss: 1.444691687822342\n",
      "Epoch 75/100, Loss: 1.4608972668647766\n",
      "Epoch 76/100, Loss: 1.4736134707927704\n",
      "Epoch 77/100, Loss: 1.4766613245010376\n",
      "Epoch 78/100, Loss: 1.4114743173122406\n",
      "Epoch 79/100, Loss: 1.4491340219974518\n",
      "Epoch 80/100, Loss: 1.476130485534668\n",
      "Epoch 81/100, Loss: 1.4269956946372986\n",
      "Epoch 82/100, Loss: 1.455125331878662\n",
      "Epoch 83/100, Loss: 1.4292406737804413\n",
      "Epoch 84/100, Loss: 1.3813532590866089\n",
      "Epoch 85/100, Loss: 1.3684105575084686\n",
      "Epoch 86/100, Loss: 1.40200674533844\n",
      "Epoch 87/100, Loss: 1.3860470056533813\n",
      "Epoch 88/100, Loss: 1.4202382862567902\n",
      "Epoch 89/100, Loss: 1.4002312421798706\n",
      "Epoch 90/100, Loss: 1.389215499162674\n",
      "Epoch 91/100, Loss: 1.371912658214569\n",
      "Epoch 92/100, Loss: 1.36455699801445\n",
      "Epoch 93/100, Loss: 1.388392150402069\n",
      "Epoch 94/100, Loss: 1.3430184721946716\n",
      "Epoch 95/100, Loss: 1.342201143503189\n",
      "Epoch 96/100, Loss: 1.3550641238689423\n",
      "Epoch 97/100, Loss: 1.2933246195316315\n",
      "Epoch 98/100, Loss: 1.3170131146907806\n",
      "Epoch 99/100, Loss: 1.2937295138835907\n",
      "Epoch 100/100, Loss: 1.3549789190292358\n"
     ]
    }
   ],
   "source": [
    "# Define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for  i , (tokens, labels) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(tokens)\n",
    "        \n",
    "        # Reshape outputs and labels to (batch_size * seq_length, output_dim)\n",
    "        outputs = outputs.view(-1, output_dim)\n",
    "        labels = labels.view(-1)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(sent):\n",
    "    lets , tashs = utils.extract_let_tash(sent)\n",
    "    let_tokens = let_tok.sent_to_ids(lets)\n",
    "    let_tokens_tensor = torch.tensor(let_tokens)\n",
    "    let_tokens_tensor\n",
    "    with torch.no_grad():\n",
    "        outs = model(let_tokens_tensor)\n",
    "        \n",
    "    max_values, max_indices =torch.max(outs, dim=1)\n",
    "    tashs_preds = tash_tok.ids_to_sent(max_indices.tolist())\n",
    "    pred=\"\".join([char+pred_tash if pred_tash!=\" \" else char for char,pred_tash in list(zip(lets , tashs_preds))])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'حدَيث عهَ حب حمَ أَنَ أَخَتَ'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = data[600]\n",
    "inference(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
